{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor Recommendation System\n",
    "\n",
    "In this project, we aim to develop a recommendation system for TripAdvisor reviews that surpasses the performance of the BM25 algorithm. We'll begin by implementing a BM25 baseline using the `rank_bm25` library and then explore advanced Natural Language Processing (NLP) techniques to enhance the recommendation system.\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "### 1.1 Importing Necessary Libraries\n",
    "\n",
    "We'll start by importing the essential libraries for data manipulation and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading the Dataset\n",
    "\n",
    "Next, we'll load the TripAdvisor Hotel Reviews dataset. Ensure that the dataset file (`Reviews.csv`) is in the same directory as this notebook or provide the correct path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('tripadvisor_hotel_reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Review', 'Rating'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Inspecting the Dataset\n",
    "\n",
    "Let's examine the first few rows of the dataset to understand its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should have the following columns:\n",
    "\n",
    "- `Review`: Concatenated reviews for the place.\n",
    "- `Rating`: Average rating of the place based on all reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Preprocessing the Reviews\n",
    "\n",
    "We'll preprocess the concatenated reviews by tokenizing the text, converting it to lowercase, removing punctuation, and eliminating stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the reviews\n",
    "df['processed_review'] = df['Review'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing the BM25 Baseline\n",
    "\n",
    "### 2.1 Installing the `rank_bm25` Library\n",
    "\n",
    "We'll install the `rank_bm25` library, which provides an efficient implementation of the BM25 algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rank_bm25) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initializing the BM25 Model\n",
    "\n",
    "We'll initialize the BM25 model with the processed reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Create a list of processed reviews\n",
    "corpus = df['processed_review'].tolist()\n",
    "\n",
    "# Initialize the BM25 model\n",
    "bm25 = BM25Okapi(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Defining a Function to Recommend Similar Places\n",
    "\n",
    "We'll define a function that, given a place's index, returns the most similar place based on BM25 similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_place(index):\n",
    "    # Get the processed review of the place\n",
    "    query = df.loc[index, 'processed_review']\n",
    "    # Compute BM25 scores\n",
    "    scores = bm25.get_scores(query)\n",
    "    # Get the index of the most similar place (excluding the query place itself)\n",
    "    scores[index] = -float('inf')  # Exclude the same place\n",
    "    most_similar_index = scores.argmax()\n",
    "    return most_similar_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating the BM25 Model\n",
    "\n",
    "To evaluate the effectiveness of the BM25 model, we'll calculate the Mean Squared Error (MSE) between the average ratings of each place and the ratings of its most similar recommended place. This comparison allows us to assess how accurately BM25 matches each place with another based on review similarity, without using any explicit rating information.\n",
    "\n",
    "We'll optimize this process by using parallel processing with `joblib`, enabling us to compute the MSE across all entries in the dataset more efficiently. By leveraging all available CPU cores, this approach significantly reduces the time required to perform the evaluation on the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Average MSE for first 5000 entries: 1.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "# Select the first 5000 indexes\n",
    "sample_indexes = df.index[:100]\n",
    "\n",
    "def compute_mse_for_index(index):\n",
    "    # Get the recommended place\n",
    "    recommended_index = recommend_similar_place(index)\n",
    "    # Get the ratings of the query and recommended places\n",
    "    query_rating = df.loc[index, 'Rating']\n",
    "    recommended_rating = df.loc[recommended_index, 'Rating']\n",
    "    # Compute MSE for the current pair\n",
    "    return mean_squared_error([query_rating], [recommended_rating])\n",
    "\n",
    "def evaluate_bm25_parallel(selected_indexes):\n",
    "    # Use Parallel to compute the MSE in parallel for the selected indexes\n",
    "    mse_scores = Parallel(n_jobs=-1)(delayed(compute_mse_for_index)(index) for index in selected_indexes)\n",
    "    # Compute the average MSE\n",
    "    average_mse = np.mean(mse_scores)\n",
    "    return average_mse\n",
    "\n",
    "# Evaluate the BM25 model on the first 5000 entries\n",
    "bm25_mse = evaluate_bm25_parallel(sample_indexes)\n",
    "print(f'BM25 Average MSE for first 5000 entries: {bm25_mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Developing an Enhanced Recommendation Model\n",
    "\n",
    "To improve on the BM25 baseline, we'll use a hybrid approach by combining BM25 with word embeddings. The goal is to create a model that better captures the semantic similarities between reviews. Specifically, we'll:\n",
    "\n",
    "1. Generate embeddings for each review using a pre-trained model.\n",
    "2. Combine BM25 similarity scores with embedding-based similarity scores for improved recommendations.\n",
    "\n",
    "This hybrid approach should ideally reduce the Mean Squared Error (MSE) compared to BM25 alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setting Up Pre-trained Embeddings\n",
    "\n",
    "We'll start by generating sentence embeddings for each review using `Sentence-BERT`. These embeddings will help capture semantic similarities between reviews, which we can then combine with BM25 similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.45.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.0.1+cu117)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.25.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anis\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each review in the dataset\n",
    "df['embedding'] = df['Review'].apply(lambda x: model.encode(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Calculating Similarity Scores\n",
    "\n",
    "We'll use cosine similarity between the embeddings to determine the semantic similarity between reviews. This similarity measure will complement the BM25 score, which is based on exact word matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_embedding_similarity(index, candidate_index):\n",
    "    # Retrieve the embeddings for both reviews\n",
    "    embedding1 = df.loc[index, 'embedding']\n",
    "    embedding2 = df.loc[candidate_index, 'embedding']\n",
    "    # Compute cosine similarity\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Hybrid Recommendation Function\n",
    "\n",
    "We'll develop a hybrid recommendation function that combines BM25 and embedding similarities to find the most similar place. We’ll use a weighted average of BM25 and embedding scores to adjust their influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid_place(index, alpha=0.5):\n",
    "    # Get the BM25 scores\n",
    "    query = df.loc[index, 'processed_review']\n",
    "    bm25_scores = bm25.get_scores(query)\n",
    "    \n",
    "    # Combine BM25 and embedding similarities for all other entries\n",
    "    hybrid_scores = []\n",
    "    for candidate_index in df.index:\n",
    "        if candidate_index != index:\n",
    "            # Get BM25 score\n",
    "            bm25_score = bm25_scores[candidate_index]\n",
    "            # Get embedding similarity\n",
    "            embedding_score = get_embedding_similarity(index, candidate_index)\n",
    "            # Hybrid score: weighted average of BM25 and embedding scores\n",
    "            hybrid_score = alpha * bm25_score + (1 - alpha) * embedding_score\n",
    "            hybrid_scores.append((candidate_index, hybrid_score))\n",
    "    \n",
    "    # Sort by hybrid score (higher is better) and return the best match\n",
    "    most_similar_index = max(hybrid_scores, key=lambda x: x[1])[0]\n",
    "    return most_similar_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the Hybrid Model\n",
    "\n",
    "We'll evaluate the hybrid model on the first 5000 entries and compare its Mean Squared Error (MSE) with that of BM25. This will help us assess if our modifications improve the recommendation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model Average MSE for first 5000 entries: 1.48\n"
     ]
    }
   ],
   "source": [
    "def evaluate_hybrid_parallel(selected_indexes=None, alpha=0.5):\n",
    "    if selected_indexes is None:\n",
    "        selected_indexes = df.index[:100]  # Default to first 5000 entries if none specified\n",
    "    mse_scores = Parallel(n_jobs=8)(delayed(lambda idx: mean_squared_error(\n",
    "        [df.loc[idx, 'Rating']],\n",
    "        [df.loc[recommend_hybrid_place(idx, alpha), 'Rating']]\n",
    "    ))(index) for index in selected_indexes)\n",
    "    # Calculate average MSE\n",
    "    average_mse = np.mean(mse_scores)\n",
    "    return average_mse\n",
    "\n",
    "# Run evaluation with alpha set to 0.5 for equal weighting\n",
    "hybrid_mse = evaluate_hybrid_parallel(sample_indexes, alpha=0.2)\n",
    "print(f'Hybrid Model Average MSE for first 5000 entries: {hybrid_mse}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
